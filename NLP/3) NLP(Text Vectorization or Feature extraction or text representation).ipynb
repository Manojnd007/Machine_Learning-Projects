{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a781ad",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "1. What is feature extraction from text or text representation or text vectorization?\n",
    "\n",
    "Converting the text into number format\n",
    "\n",
    "2. Why do we need it?\n",
    "\n",
    "We need to create the feature.\n",
    "\n",
    "eg: Garbeg in Garbeg out, so we needto get the insight from the data and create good feature.\n",
    "\n",
    "3. Why it is difficult ?\n",
    "\n",
    "Converting image to number is easy(Image formed with pixel size 255 which is number), speech recognization is also easy.\n",
    "Textual data it is difficult since people do spelling mistake and short forms etc...\n",
    "\n",
    "4. What is the core idea and the techniques.\n",
    "\n",
    "We have to get the simentic meaning of the text meaning.\n",
    "\n",
    "Techniques: \n",
    "1. 1 hot encoding.\n",
    "2. Bag of words.\n",
    "3. ngrams.\n",
    "4. tfidf.\n",
    "5. custom features.\n",
    "6. Embedding ( word2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2558b48",
   "metadata": {},
   "source": [
    "### Common Terms\n",
    "\n",
    "1. Corpus (C) : Combinarion of all the words in the entire data set which contains repeated word as well.\n",
    "\n",
    "2. Vocabulary (V) : count of unique number of words which is used in the corpus.\n",
    "\n",
    "3. Document (D) : Each individual review is called as document. \n",
    "\n",
    "4. Word (W) : Each document individual word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a25f1",
   "metadata": {},
   "source": [
    "#### 1 Hot Encoding\n",
    " Converting the each word into v dimentional vector.\n",
    "\n",
    "#### This techniques is not used in the present world.\n",
    "\n",
    "Pros:\n",
    "1. Very Intevitive.\n",
    "2. Easy to train the examples.\n",
    "\n",
    "Cons:\n",
    "1. Sparcity is the major problem which makes training overfitting.\n",
    "2. Required more and more data to avoid (OOV) out of vocabolary.\n",
    "3. No fixed size.\n",
    "4. No capturing of simantic meaning this is the major disadvantage of the one hot encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6a43f",
   "metadata": {},
   "source": [
    "### Bag of words.\n",
    "\n",
    "#### Most used in text classifications.\n",
    "\n",
    "1. Creating the vocabolary with the text present in the corpus.\n",
    "2. Creating the document with the vocabolary.\n",
    "3. In the bag of words order of the words doesn't matter .\n",
    "4. We convert into vectorization then find the cosine distance between the vectors to find the predictions.\n",
    "\n",
    "#### Advantage:\n",
    "1. Simple and intevitive.\n",
    "2. handles the OOV(Out of Vocabolary) and fixed size will be maintained.\n",
    "3. We can capture better simantic meaning compare to One hot encoding\n",
    "\n",
    "#### Disadvantage:\n",
    "1. Sparcity and over fitting.\n",
    "2. OOV is present since we are ignoring if we find any oov.\n",
    "3. We are ignoring the ordering (This is the major disadvantage), if we change the order the meaning of the sentance will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f781b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c2d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implimentation of bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11050a",
   "metadata": {},
   "source": [
    "#####  Hyper parameter tuning\n",
    "1. lower_case =True (will convert the words into lower case)\n",
    "2. stop_words = we can remove stop_words from the language\n",
    "3. binary = True(Most used in Sentiment Analysis).\n",
    "4. max_features = To remove the rare words we use this and it's like experiment since it's a hyper parameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ef1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'columns':['Hi My Name is Manoj','I work as a Generalized Data Scientist']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a272b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9ee9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi My Name is Manoj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work as a Generalized Data Scientist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  columns\n",
       "0                     Hi My Name is Manoj\n",
       "1  I work as a Generalized Data Scientist"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27707b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "\n",
    "bow = cv.fit_transform(df['columns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d756e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 1 1 1 0 0]]\n",
      "[[1 1 1 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "#We just found the 2 vectorized array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82204b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi': 3, 'my': 6, 'name': 7, 'is': 4, 'manoj': 5, 'work': 9, 'as': 0, 'generalized': 2, 'data': 1, 'scientist': 8}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee46db",
   "metadata": {},
   "source": [
    "### Bag of N-Grams\n",
    "\n",
    "This is quit similary to bag of words.\n",
    "\n",
    "Advantage:\n",
    "We combine multiple words to form the vocabolary.\n",
    "\n",
    "\n",
    "combination of 2 words is bi-gram.\n",
    "combination of 3 words is tri-gram.\n",
    "etc...\n",
    "eg: For Bi-gram => ['Hi my','my name','name is','is manoj']\n",
    "eg: for tri-gram => ['Hi my name','name is manoj']\n",
    "\n",
    "\n",
    "Benefit:\n",
    "1. Able to capture semantic meaning of the sentence using n-gram\n",
    "2. Easy to impliment.\n",
    "\n",
    "Disadvantage:\n",
    "1. The dimention of the vocabulary will be increased since number of features will be increased\n",
    "2. Out of Vocabulary is still the issue since it will not consider the OOV as in bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75ff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implimentation of n-grams\n",
    "\n",
    "### Implimentation of bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2)) # unigram and Bigram\n",
    "#cv = CountVectorizer(ngram_range=(2,2)) # Only Bigram\n",
    "#cv = CountVectorizer(ngram_range=(3,3)) # Only trigram\n",
    "#cv = CountVectorizer(ngram_range=(1,3)) #uni, bi and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c002f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram = cv.fit_transform(df['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b4dc931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 6,\n",
       " 'my': 11,\n",
       " 'name': 13,\n",
       " 'is': 8,\n",
       " 'manoj': 10,\n",
       " 'hi my': 7,\n",
       " 'my name': 12,\n",
       " 'name is': 14,\n",
       " 'is manoj': 9,\n",
       " 'work': 16,\n",
       " 'as': 0,\n",
       " 'generalized': 4,\n",
       " 'data': 2,\n",
       " 'scientist': 15,\n",
       " 'work as': 17,\n",
       " 'as generalized': 1,\n",
       " 'generalized data': 5,\n",
       " 'data scientist': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "#We saw bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652ce31",
   "metadata": {},
   "source": [
    "### TFIDF : TERM FREQUENCY INVERSE DOCUMENT FREQUENCY.\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents.\n",
    "\n",
    "    (NUMBER OF OCCURANCE OF TERM T IN DOCUMENT D)\n",
    "TF =--------------------------------------------\n",
    "    (TOTAL NUMBER OF TERMS IN THE DOCUMENT D)\n",
    "    \n",
    "                      (TOTAL NUMBER OF DUCUMENTS IN THE CORPUS)\n",
    "IDF = LOG to base n ------------------------------------------\n",
    "                       (NUMBER OF DOCUMENTS WITH TERM T IN THEM)\n",
    "   \n",
    "   Why log is used in TDF:  We minimise the large number into a smaller number normalization with the help of log\n",
    "   eg: without log we have 100000000, with log we get 1000, simple.\n",
    "   \n",
    " total tfidf = tf * idf\n",
    "\n",
    "\n",
    "Advantage:\n",
    "1. Main useful for the information reterival eg: search reterval\n",
    "\n",
    "Disadvantage:\n",
    "1. Sparcity is more.\n",
    "2. OOV.\n",
    "3. Dimentionality will be increased so it needs more computational power and makes over fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1633d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , 0.4472136, 0.4472136, 0.4472136,\n",
       "        0.4472136, 0.4472136, 0.       , 0.       ],\n",
       "       [0.4472136, 0.4472136, 0.4472136, 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.4472136, 0.4472136]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(df['columns']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bff7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
      " 1.40546511 1.40546511 1.40546511 1.40546511]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)\n",
    "#rint(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc40eeb",
   "metadata": {},
   "source": [
    "### Custom features\n",
    "\n",
    "Preparing our Own feature is called custom featues.\n",
    "\n",
    "We create the data set and attributes to do feature extraction and selection.\n",
    "1. Number of positive words in the review\n",
    "2. Number of negative words in the review\n",
    "3. Ratio of positive words and negative words.\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd0813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb89aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
